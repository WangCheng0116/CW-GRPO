# Model arguments
model_name_or_path: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
model_revision: main
torch_dtype: bfloat16
attn_implementation: flash_attention_2

# Data training arguments
dataset_name: knoveleng/open-rs
system_prompt: "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer, and put your final answer within \\boxed{{}} . The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. Note that respond by English, NOT use other languages."

# GRPO trainer config
bf16: true
remove_unused_columns: False
use_vllm: true
vllm_device: auto
vllm_enforce_eager: true
vllm_gpu_memory_utilization: 0.7
vllm_max_model_len: 4096
do_eval: false
gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
hub_model_id: Gaussian-1.5B-Format-New-Repro
hub_strategy: every_save
learning_rate: 1.0e-06
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
lr_scheduler_type: cosine_with_min_lr
lr_scheduler_kwargs:
  min_lr_rate: 0.1
max_prompt_length: 512
max_completion_length: 4096
max_steps: 300
num_generations: 12
num_train_epochs: 1
output_dir: data/Gaussian-1.5B-Format-New-Repro
overwrite_output_dir: true
per_device_eval_batch_size: 12
per_device_train_batch_size: 12
push_to_hub: true
report_to:
- wandb
reward_funcs:
- accuracy
- format
reward_weights:
- 1.0
- 1.0
save_strategy: "steps"
save_steps: 50
seed: 2025
temperature: 0.7
warmup_ratio: 0.1
use_ours: true

# # Model arguments
# model_name_or_path: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
# model_revision: main
# torch_dtype: bfloat16
# attn_implementation: flash_attention_2

# # Data training arguments
# dataset_name: knoveleng/open-rs
# system_prompt: "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer, and put your final answer within \\boxed{{}} . The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. Note that respond by English, NOT use other languages."

# # GRPO trainer config
# bf16: true
# remove_unused_columns: False
# use_vllm: true
# vllm_device: auto
# vllm_enforce_eager: true
# vllm_gpu_memory_utilization: 0.7
# vllm_max_model_len: 4096
# do_eval: false
# gradient_accumulation_steps: 4
# gradient_checkpointing: true
# gradient_checkpointing_kwargs:
#   use_reentrant: false
# hub_model_id: GRPO-1.5B-Format-New
# hub_strategy: every_save
# learning_rate: 1.0e-06
# log_completions: true
# log_level: info
# logging_first_step: true
# logging_steps: 1
# logging_strategy: steps
# lr_scheduler_type: cosine_with_min_lr
# lr_scheduler_kwargs:
#   min_lr_rate: 0.1
# max_prompt_length: 512
# max_completion_length: 4096
# max_steps: 500
# num_generations: 10
# num_train_epochs: 1
# output_dir: data/GRPO-1.5B-Format-New
# overwrite_output_dir: true
# per_device_eval_batch_size: 10
# per_device_train_batch_size: 10
# push_to_hub: true
# report_to:
# - wandb
# reward_funcs:
# - accuracy
# - format
# reward_weights:
# - 1.0
# - 1.0
# save_strategy: "steps"
# save_steps: 50
# seed: 2025
# temperature: 0.7
# warmup_ratio: 0.1
# use_ours: false